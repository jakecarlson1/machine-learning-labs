{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Extending Logistic Regression\n",
    "\n",
    "### Eric Smith and Jake Carlson\n",
    "\n",
    "## Introduction\n",
    "For this lab, we will again be examining the Global Terrorism Database maintained by the National Consortium for the Study of Terrorism and Responses to Terrorism (START) at the University of Maryland. We will be looking at attacks that happened in the United States over the whole time span of the data set, since it's creation in 1974.\n",
    "\n",
    "We will be extending binary logistic regression based classification to support multiple classes. Specifically, based on the attributes of the input entity, we want to predict what the attack type of the entity is.\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "### Motivations\n",
    "Protecting the United States from terror threats has been a major objective of the federal government. This is characterized by the founding of the Department of Homeland Security in 2001. But predicting when an attack will happen based on certain attributes is next to impossible. Attempting to train a model on the Global Terrorism Database to learn when terrorist attacks happen will result in a model that is over-trained on the GTD and will fail to predict any such attacks. Not to mention, such a system would have to be accompanied by a large-scale communication monitoring and processing system capable of feeding the model relevant inputs that exemplify a possible attack.\n",
    "\n",
    "Instead of trying to predict when an attack will happen, our goal is to create a model that can predict the cost associate with an individual attack. Immediately after an attack has happened, law enforcement can feed in information about the attack, such as the attack type, the number of people injured, and the target type, and they could receive an approximation of the amount of property damage dealt to their city. Such a model would allow city officials and law enforcement to estimate in real time how much an attack will cost their city. Knowing the estimated cost would enable city officials to determine if they need to request support from the federal government in a shorter timeframe. Furthermore, cities could plan their future budgets accordingly to incorporate funding in response to a terrorist attack.\n",
    "\n",
    "Cities have to submit requests to FEMA for non-disaster grants to aid in the prevention and response to terrorist activity. The Department of Homeland Security can also issue grants to aid in the prevention of terrorism. Grant policies start with Congress allocating funds for federal grants of this type. The Executive Branch provides input for how the policy should be implemented. Then grant issuing agencies develop their own policies for how to allocate grant money.\n",
    "\n",
    "Each state defines their own thresholds for when an attack is severe enough that they will ask for federal assistance. Our model will allow officials to immediately decide if they need to file for a federal grant. Smaller cities have lwoer thresholds and larger cities can handle higher costs before needing assistance.\n",
    "\n",
    "### Objectives\n",
    "Based on the characteristics of an attack, such as the target type and the date, we want to assign an estimated cost label to the entity. Because our system will be used to estimate the cost for local city governements, perfect classification of cost is not required. However, it is important that these estimations are as accurate as possible because a request for a grant will need to be formed and sent to the federal government.\n",
    "\n",
    "...\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Attributes\n",
    "Here is the list of attributes we will keep in our data set to use for classification.\n",
    "\n",
    "#### General Information\n",
    "- **iyear** (ordinal): The year the event occured in\n",
    "- **imonth** (ordinal): The month the event occured in\n",
    "- **iday** (ordinal): The day the event occured in\n",
    "- **extended** (binary): 1 if the incident was longer than 24 hours, 0 otherwise\n",
    "    - **resolution** (ordinal): The date an extended incident was resolved if *extended* is 1\n",
    "\n",
    "\n",
    "- **inclusion criteria** (binary): There are three inclusion criteria where a 1 indicates the event meets that criteria\n",
    "    - **crit1**: Political, economic, religious, or social goal\n",
    "    - **crit2**: Intention to coerce, intimidate, or publicize\n",
    "    - **crit3**: Outside international humanitarian law\n",
    "\n",
    "\n",
    "#### Location\n",
    "We will provide the name of the city to the model. An alternative method would be to train a unique logistic regression algorithm for each city where our system is deployed.\n",
    "- **city** (text): Name of the city in which the event occured\n",
    "- **vicinity** (nominal/binary): A 1 indicates the event occured in the immediate vicinity of *city*, 0 indicates the even occured in *city*\n",
    "- **latitude** (ratio): The latitude of the *city* in which the event occured\n",
    "- **longitude** (ratio): The longitude of the *city* in which the event occured\n",
    "\n",
    "#### Attack Type\n",
    "The most severe method of attack. This will be our class label. Although the original data set contains columns for three different attack types, the attack types are ranked by their severity. Many attacks only have one attack type. By removing the second and third attack types from our data set, we will still be predicting the most severe of the attack types.\n",
    "- **attacktype1** (ordinal): Most severe attack type\n",
    "\n",
    "- The attack types follow the following hierarchy:\n",
    "    1. Assassination\n",
    "    2. Armed Assault\n",
    "    3. Bombing/Explosion\n",
    "    4. Hijacking\n",
    "    5. Barricade Incident\n",
    "    6. Kidnapping\n",
    "    7. Facility/Infrastructure Attack \n",
    "    8. Unarmed Assault\n",
    "    9. Unknown\n",
    "\n",
    "\n",
    "- **suicide** (nominal/binary): A 1 indicates there was evidence the attacker did not make an effort to escape with their life\n",
    "\n",
    "#### Target Type\n",
    "We will only be considering the first target type of the attack. The set of target attributes is provided below:\n",
    "- **targtype1, targtype1_txt** (nominal): The general type of target from the following list:\n",
    "    1. Business\n",
    "    2. Government (General)\n",
    "    3. Police\n",
    "    4. Military\n",
    "    5. Abortion related\n",
    "    6. Airports and aircraft\n",
    "    7. Government (Diplomatic)\n",
    "    8. Educational institution\n",
    "    9. Food or water supply\n",
    "    10. Journalists and media\n",
    "    11. Maritime\n",
    "    12. NGO\n",
    "    13. Other\n",
    "    14. Private citizens and property\n",
    "    15. Religious figures and institutions\n",
    "    16. Telecommunication\n",
    "    17. Terrirists and non-state militias\n",
    "    18. Tourists\n",
    "    19. Transportation\n",
    "    20. Unknown\n",
    "    21. Utilities\n",
    "    22. Violent political parties\n",
    "    \n",
    "\n",
    "- ? **targsubtype1, targsubtype1_txt** (nominal): There are a number of subtypes for each of the above target types\n",
    "\n",
    "#### Perpetrator Information\n",
    "The data set provides information on up to three perpetrators if the attack was conducted by multiple groups. We will only be considering the first group, or the one decided to have the most responsibility for the attack.\n",
    "- **individual** (binary): A 1 indicates the individuals carrying out the attack are not affiliated with a terror organization\n",
    "- **nperps** (ratio): Indicates the total number of terrorists participating in the event\n",
    "- **nperpcap** (ratio): Number of perpatrators taken into custody\n",
    "- **claimed** (binary): A 1 indicates a person or group claimed responsibility for the attack\n",
    "- **claimmode** (nominal): Records the method the terror group used to claim responsibility for the attack. Can be one of the ten following categories:\n",
    "    1. Letter\n",
    "    2. Call (post-incident)\n",
    "    3. Call (pre-incident)\n",
    "    4. E-mail\n",
    "    5. Note left at scene\n",
    "    6. Video\n",
    "    7. Posted to website\n",
    "    8. Personal claim\n",
    "    9. Other\n",
    "    10. Unknown\n",
    "\n",
    "\n",
    "#### Casualties and Consequences\n",
    "- **nkill** (ratio): Records the number of confirmed kills for the incident\n",
    "- **nkillter** (ratio): Indicates the number of terrorists who were killed in the event\n",
    "- **nwound** (ratio): Indicates the number of people who sustained non-fatal injuries in the event\n",
    "- **nwoundte** (ratio): Indicates the number of terrorists who sustained non-lethal injuries\n",
    "- **property** (binary): A 1 indicates the event resulted in property damage. We will only select entities that resulted in property damage\n",
    "- **propextent** (ordinal): If *property* is a 1, this field records the extent of the property damage following the scheme:\n",
    "    <ol start='0'>\n",
    "        <li>Catastrophic (likely > \\$1 billion)</li>\n",
    "        <li>Major (likely > \\$1 million and < \\$1 billion)</li>\n",
    "        <li>Minor (likely < \\$1 million)</li>\n",
    "        <li>Unknown</li>\n",
    "    </ol>\n",
    "\n",
    "### Data Cleaning\n",
    "We will clean the data set so only the above attributes are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventid</th>\n",
       "      <th>iyear</th>\n",
       "      <th>imonth</th>\n",
       "      <th>iday</th>\n",
       "      <th>approxdate</th>\n",
       "      <th>extended</th>\n",
       "      <th>resolution</th>\n",
       "      <th>country</th>\n",
       "      <th>country_txt</th>\n",
       "      <th>region</th>\n",
       "      <th>...</th>\n",
       "      <th>addnotes</th>\n",
       "      <th>scite1</th>\n",
       "      <th>scite2</th>\n",
       "      <th>scite3</th>\n",
       "      <th>dbsource</th>\n",
       "      <th>INT_LOG</th>\n",
       "      <th>INT_IDEO</th>\n",
       "      <th>INT_MISC</th>\n",
       "      <th>INT_ANY</th>\n",
       "      <th>related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197001010002</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>The Cairo Chief of Police, William Petersen, r...</td>\n",
       "      <td>\"Police Chief Quits,\" Washington Post, January...</td>\n",
       "      <td>\"Cairo Police Chief Quits; Decries Local 'Mili...</td>\n",
       "      <td>Christopher Hewitt, \"Political Violence and Te...</td>\n",
       "      <td>Hewitt Project</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197001020002</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Damages were estimated to be between $20,000-$...</td>\n",
       "      <td>Committee on Government Operations United Stat...</td>\n",
       "      <td>Christopher Hewitt, \"Political Violence and Te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hewitt Project</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197001020003</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>The New Years Gang issue a communiqu to a loc...</td>\n",
       "      <td>Tom Bates, \"Rads: The 1970 Bombing of the Army...</td>\n",
       "      <td>David Newman, Sandra Sutherland, and Jon Stewa...</td>\n",
       "      <td>The Wisconsin Cartographers' Guild, \"Wisconsin...</td>\n",
       "      <td>Hewitt Project</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197001030001</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Karl Armstrong's girlfriend, Lynn Schultz, dro...</td>\n",
       "      <td>Committee on Government Operations United Stat...</td>\n",
       "      <td>Tom Bates, \"Rads: The 1970 Bombing of the Army...</td>\n",
       "      <td>David Newman, Sandra Sutherland, and Jon Stewa...</td>\n",
       "      <td>Hewitt Project</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197001050001</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PGIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        eventid  iyear  imonth  iday approxdate  extended resolution  country  \\\n",
       "0  197001010002   1970       1     1        NaN         0        NaN      217   \n",
       "1  197001020002   1970       1     2        NaN         0        NaN      217   \n",
       "2  197001020003   1970       1     2        NaN         0        NaN      217   \n",
       "3  197001030001   1970       1     3        NaN         0        NaN      217   \n",
       "4  197001050001   1970       1     1        NaN         0        NaN      217   \n",
       "\n",
       "     country_txt  region   ...     \\\n",
       "0  United States       1   ...      \n",
       "1  United States       1   ...      \n",
       "2  United States       1   ...      \n",
       "3  United States       1   ...      \n",
       "4  United States       1   ...      \n",
       "\n",
       "                                            addnotes  \\\n",
       "0  The Cairo Chief of Police, William Petersen, r...   \n",
       "1  Damages were estimated to be between $20,000-$...   \n",
       "2  The New Years Gang issue a communiqu to a loc...   \n",
       "3  Karl Armstrong's girlfriend, Lynn Schultz, dro...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              scite1  \\\n",
       "0  \"Police Chief Quits,\" Washington Post, January...   \n",
       "1  Committee on Government Operations United Stat...   \n",
       "2  Tom Bates, \"Rads: The 1970 Bombing of the Army...   \n",
       "3  Committee on Government Operations United Stat...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              scite2  \\\n",
       "0  \"Cairo Police Chief Quits; Decries Local 'Mili...   \n",
       "1  Christopher Hewitt, \"Political Violence and Te...   \n",
       "2  David Newman, Sandra Sutherland, and Jon Stewa...   \n",
       "3  Tom Bates, \"Rads: The 1970 Bombing of the Army...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              scite3        dbsource  INT_LOG  \\\n",
       "0  Christopher Hewitt, \"Political Violence and Te...  Hewitt Project       -9   \n",
       "1                                                NaN  Hewitt Project       -9   \n",
       "2  The Wisconsin Cartographers' Guild, \"Wisconsin...  Hewitt Project        0   \n",
       "3  David Newman, Sandra Sutherland, and Jon Stewa...  Hewitt Project        0   \n",
       "4                                                NaN            PGIS        0   \n",
       "\n",
       "   INT_IDEO INT_MISC INT_ANY  related  \n",
       "0        -9        0      -9      NaN  \n",
       "1        -9        0      -9      NaN  \n",
       "2         0        0       0      NaN  \n",
       "3         0        0       0      NaN  \n",
       "4         0        0       0      NaN  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../Lab1/data/us_only.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['eventid', 'iyear', 'imonth', 'iday', 'approxdate', 'extended',\n",
       "       'resolution', 'country', 'country_txt', 'region', 'region_txt',\n",
       "       'provstate', 'city', 'latitude', 'longitude', 'specificity',\n",
       "       'vicinity', 'location', 'summary', 'crit1', 'crit2', 'crit3',\n",
       "       'doubtterr', 'alternative', 'alternative_txt', 'multiple',\n",
       "       'success', 'suicide', 'attacktype1', 'attacktype1_txt',\n",
       "       'attacktype2', 'attacktype2_txt', 'attacktype3', 'attacktype3_txt',\n",
       "       'targtype1', 'targtype1_txt', 'targsubtype1', 'targsubtype1_txt',\n",
       "       'corp1', 'target1', 'natlty1', 'natlty1_txt', 'targtype2',\n",
       "       'targtype2_txt', 'targsubtype2', 'targsubtype2_txt', 'corp2',\n",
       "       'target2', 'natlty2', 'natlty2_txt', 'targtype3', 'targtype3_txt',\n",
       "       'targsubtype3', 'targsubtype3_txt', 'corp3', 'target3', 'natlty3',\n",
       "       'natlty3_txt', 'gname', 'gsubname', 'gname2', 'gsubname2', 'gname3',\n",
       "       'gsubname3', 'motive', 'guncertain1', 'guncertain2', 'guncertain3',\n",
       "       'individual', 'nperps', 'nperpcap', 'claimed', 'claimmode',\n",
       "       'claimmode_txt', 'claim2', 'claimmode2', 'claimmode2_txt', 'claim3',\n",
       "       'claimmode3', 'claimmode3_txt', 'compclaim', 'weaptype1',\n",
       "       'weaptype1_txt', 'weapsubtype1', 'weapsubtype1_txt', 'weaptype2',\n",
       "       'weaptype2_txt', 'weapsubtype2', 'weapsubtype2_txt', 'weaptype3',\n",
       "       'weaptype3_txt', 'weapsubtype3', 'weapsubtype3_txt', 'weaptype4',\n",
       "       'weaptype4_txt', 'weapsubtype4', 'weapsubtype4_txt', 'weapdetail',\n",
       "       'nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', 'nwoundte',\n",
       "       'property', 'propextent', 'propextent_txt', 'propvalue',\n",
       "       'propcomment', 'ishostkid', 'nhostkid', 'nhostkidus', 'nhours',\n",
       "       'ndays', 'divert', 'kidhijcountry', 'ransom', 'ransomamt',\n",
       "       'ransomamtus', 'ransompaid', 'ransompaidus', 'ransomnote',\n",
       "       'hostkidoutcome', 'hostkidoutcome_txt', 'nreleased', 'addnotes',\n",
       "       'scite1', 'scite2', 'scite3', 'dbsource', 'INT_LOG', 'INT_IDEO',\n",
       "       'INT_MISC', 'INT_ANY', 'related'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent maintained:  73.09644670050761 %\n"
     ]
    }
   ],
   "source": [
    "# drop rows without property damage or unknown city\n",
    "orig_len = df.shape[0]\n",
    "df = df[df['property'] == 1]\n",
    "df = df[df['city'] != \"Unknown\"]\n",
    "new_len = df.shape[0]\n",
    "print(\"Percent maintained: \", new_len/orig_len*100, \"%\")\n",
    "\n",
    "# select columns of interest\n",
    "df = df[['iyear', 'imonth', 'iday', 'extended', 'vicinity',\n",
    "         'latitude', 'longitude', 'crit1', 'crit2', 'crit3', 'suicide',\n",
    "         'attacktype1', 'targtype1', 'individual', 'nperps', 'nperpcap', \n",
    "         'claimed', 'claimmode', 'nkill', 'nkillter', 'nwound', \n",
    "         'nwoundte', 'propextent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iyear</th>\n",
       "      <th>imonth</th>\n",
       "      <th>iday</th>\n",
       "      <th>extended</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>crit1</th>\n",
       "      <th>crit2</th>\n",
       "      <th>crit3</th>\n",
       "      <th>...</th>\n",
       "      <th>individual</th>\n",
       "      <th>nperps</th>\n",
       "      <th>nperpcap</th>\n",
       "      <th>claimed</th>\n",
       "      <th>claimmode</th>\n",
       "      <th>nkill</th>\n",
       "      <th>nkillter</th>\n",
       "      <th>nwound</th>\n",
       "      <th>nwoundte</th>\n",
       "      <th>propextent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.005105</td>\n",
       "      <td>-89.176269</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.805065</td>\n",
       "      <td>-122.273024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.076592</td>\n",
       "      <td>-89.412488</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.072950</td>\n",
       "      <td>-89.386694</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.740010</td>\n",
       "      <td>-104.992259</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iyear  imonth  iday  extended  vicinity   latitude   longitude  crit1  \\\n",
       "0   1970       1     1         0         0  37.005105  -89.176269      1   \n",
       "1   1970       1     2         0         0  37.805065 -122.273024      1   \n",
       "2   1970       1     2         0         0  43.076592  -89.412488      1   \n",
       "3   1970       1     3         0         0  43.072950  -89.386694      1   \n",
       "5   1970       1     6         0         0  39.740010 -104.992259      1   \n",
       "\n",
       "   crit2  crit3     ...      individual  nperps  nperpcap  claimed  claimmode  \\\n",
       "0      1      1     ...               0   -99.0     -99.0      0.0        NaN   \n",
       "1      1      1     ...               0   -99.0     -99.0      0.0        NaN   \n",
       "2      1      1     ...               0     1.0       1.0      1.0        1.0   \n",
       "3      1      1     ...               0     1.0       1.0      0.0        NaN   \n",
       "5      1      1     ...               0   -99.0     -99.0      0.0        NaN   \n",
       "\n",
       "   nkill  nkillter  nwound  nwoundte  propextent  \n",
       "0    0.0       0.0     0.0       0.0         3.0  \n",
       "1    0.0       0.0     0.0       0.0         3.0  \n",
       "2    0.0       0.0     0.0       0.0         3.0  \n",
       "3    0.0       0.0     0.0       0.0         3.0  \n",
       "5    0.0       0.0     0.0       0.0         3.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2016 entries, 0 to 2756\n",
      "Data columns (total 23 columns):\n",
      "iyear          2016 non-null int64\n",
      "imonth         2016 non-null int64\n",
      "iday           2016 non-null int64\n",
      "extended       2016 non-null int64\n",
      "vicinity       2016 non-null int64\n",
      "latitude       2016 non-null float64\n",
      "longitude      2016 non-null float64\n",
      "crit1          2016 non-null int64\n",
      "crit2          2016 non-null int64\n",
      "crit3          2016 non-null int64\n",
      "suicide        2016 non-null int64\n",
      "attacktype1    2016 non-null int64\n",
      "targtype1      2016 non-null int64\n",
      "individual     2016 non-null int64\n",
      "nperps         1191 non-null float64\n",
      "nperpcap       1153 non-null float64\n",
      "claimed        1155 non-null float64\n",
      "claimmode      339 non-null float64\n",
      "nkill          1952 non-null float64\n",
      "nkillter       1175 non-null float64\n",
      "nwound         1936 non-null float64\n",
      "nwoundte       1165 non-null float64\n",
      "propextent     1484 non-null float64\n",
      "dtypes: float64(11), int64(12)\n",
      "memory usage: 378.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iyear</th>\n",
       "      <th>extended</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>crit1</th>\n",
       "      <th>crit2</th>\n",
       "      <th>crit3</th>\n",
       "      <th>suicide</th>\n",
       "      <th>individual</th>\n",
       "      <th>...</th>\n",
       "      <th>claimmode_1</th>\n",
       "      <th>claimmode_2</th>\n",
       "      <th>claimmode_3</th>\n",
       "      <th>claimmode_4</th>\n",
       "      <th>claimmode_5</th>\n",
       "      <th>claimmode_6</th>\n",
       "      <th>claimmode_7</th>\n",
       "      <th>claimmode_8</th>\n",
       "      <th>claimmode_9</th>\n",
       "      <th>claimmode_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.406195</td>\n",
       "      <td>0.261021</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.423262</td>\n",
       "      <td>0.135239</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.535729</td>\n",
       "      <td>0.260123</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.535651</td>\n",
       "      <td>0.260221</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.464543</td>\n",
       "      <td>0.200913</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iyear  extended  vicinity  latitude  longitude  crit1  crit2  crit3  \\\n",
       "0    0.0     False     False  0.406195   0.261021   True   True   True   \n",
       "1    0.0     False     False  0.423262   0.135239   True   True   True   \n",
       "2    0.0     False     False  0.535729   0.260123   True   True   True   \n",
       "3    0.0     False     False  0.535651   0.260221   True   True   True   \n",
       "5    0.0     False     False  0.464543   0.200913   True   True   True   \n",
       "\n",
       "   suicide  individual      ...       claimmode_1  claimmode_2  claimmode_3  \\\n",
       "0    False       False      ...                 0            0            0   \n",
       "1    False       False      ...                 0            0            0   \n",
       "2    False       False      ...                 1            0            0   \n",
       "3    False       False      ...                 0            0            0   \n",
       "5    False       False      ...                 0            0            0   \n",
       "\n",
       "   claimmode_4  claimmode_5  claimmode_6  claimmode_7  claimmode_8  \\\n",
       "0            0            1            0            0            0   \n",
       "1            0            1            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            1            0            0            0   \n",
       "5            0            1            0            0            0   \n",
       "\n",
       "   claimmode_9  claimmode_10  \n",
       "0            0             0  \n",
       "1            0             0  \n",
       "2            0             0  \n",
       "3            0             0  \n",
       "5            0             0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "logical_cols = ['extended', 'vicinity', 'crit1', 'crit2', 'crit3',\n",
    "                'suicide', 'individual', 'claimed']\n",
    "categorical_cols = ['attacktype1', 'targtype1', 'claimmode']\n",
    "ratio_cols = ['latitude', 'longitude', 'nperps', 'nperpcap', 'nkill',\n",
    "              'nkillter', 'nwound', 'nwoundte']\n",
    "\n",
    "# replace unknowns with nan\n",
    "logical_replace = dict((l, {-9:np.nan}) for l in ['claimed'])\n",
    "ratio_replace = dict((r, {-99:np.nan, -9:np.nan}) for r in ratio_cols)\n",
    "df.replace(to_replace=logical_replace, inplace=True)\n",
    "df.replace(to_replace=ratio_replace, inplace=True)\n",
    "\n",
    "# might want to remove outliers here\n",
    "\n",
    "# replace NA's with median for col\n",
    "df.fillna(value=df.median(), inplace=True)\n",
    "\n",
    "# convert logical cols to bools\n",
    "for l in logical_cols:\n",
    "    df[l] = df[l].astype('bool')\n",
    "\n",
    "# normalize ratio cols\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df[ratio_cols] = min_max_scaler.fit_transform(df[ratio_cols])\n",
    "\n",
    "# standardize date attributes\n",
    "# use year, month, and day to get day number in year\n",
    "day_list = []\n",
    "for r in df[['iyear', 'imonth', 'iday']].iterrows():\n",
    "    # fudge day 0 to 1\n",
    "    if r[1].iday == 0:\n",
    "        day_list.append(\n",
    "            datetime(r[1].iyear, r[1].imonth, 1).timetuple().tm_yday)\n",
    "    else:\n",
    "        day_list.append(\n",
    "            datetime(r[1].iyear, r[1].imonth, r[1].iday).timetuple().tm_yday)\n",
    "        \n",
    "df = df.assign(dayn=day_list)\n",
    "\n",
    "# drop month and day attributes\n",
    "df.drop(['imonth', 'iday'], axis=1, inplace=True)\n",
    "\n",
    "# normalize day number and year col\n",
    "df['iyear'] = df['iyear'].astype(np.float64)\n",
    "df['dayn'] = df['dayn'].astype(np.float64)\n",
    "df[['iyear', 'dayn']] = min_max_scaler.fit_transform(df[['iyear', 'dayn']])\n",
    "\n",
    "# convert categorical cols as ints\n",
    "for c in categorical_cols:\n",
    "    df[c] = df[c].astype(np.int64)\n",
    "\n",
    "# one-hot encode categorical cols\n",
    "df = pd.get_dummies(df, prefix=categorical_cols, columns=categorical_cols)\n",
    "\n",
    "# zero-index propextent\n",
    "df.propextent = df.propextent - 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2016 entries, 0 to 2756\n",
      "Data columns (total 60 columns):\n",
      "iyear            2016 non-null float64\n",
      "extended         2016 non-null bool\n",
      "vicinity         2016 non-null bool\n",
      "latitude         2016 non-null float64\n",
      "longitude        2016 non-null float64\n",
      "crit1            2016 non-null bool\n",
      "crit2            2016 non-null bool\n",
      "crit3            2016 non-null bool\n",
      "suicide          2016 non-null bool\n",
      "individual       2016 non-null bool\n",
      "nperps           2016 non-null float64\n",
      "nperpcap         2016 non-null float64\n",
      "claimed          2016 non-null bool\n",
      "nkill            2016 non-null float64\n",
      "nkillter         2016 non-null float64\n",
      "nwound           2016 non-null float64\n",
      "nwoundte         2016 non-null float64\n",
      "propextent       2016 non-null float64\n",
      "dayn             2016 non-null float64\n",
      "attacktype1_1    2016 non-null uint8\n",
      "attacktype1_2    2016 non-null uint8\n",
      "attacktype1_3    2016 non-null uint8\n",
      "attacktype1_4    2016 non-null uint8\n",
      "attacktype1_5    2016 non-null uint8\n",
      "attacktype1_6    2016 non-null uint8\n",
      "attacktype1_7    2016 non-null uint8\n",
      "attacktype1_8    2016 non-null uint8\n",
      "attacktype1_9    2016 non-null uint8\n",
      "targtype1_1      2016 non-null uint8\n",
      "targtype1_2      2016 non-null uint8\n",
      "targtype1_3      2016 non-null uint8\n",
      "targtype1_4      2016 non-null uint8\n",
      "targtype1_5      2016 non-null uint8\n",
      "targtype1_6      2016 non-null uint8\n",
      "targtype1_7      2016 non-null uint8\n",
      "targtype1_8      2016 non-null uint8\n",
      "targtype1_9      2016 non-null uint8\n",
      "targtype1_10     2016 non-null uint8\n",
      "targtype1_11     2016 non-null uint8\n",
      "targtype1_12     2016 non-null uint8\n",
      "targtype1_13     2016 non-null uint8\n",
      "targtype1_14     2016 non-null uint8\n",
      "targtype1_15     2016 non-null uint8\n",
      "targtype1_16     2016 non-null uint8\n",
      "targtype1_17     2016 non-null uint8\n",
      "targtype1_18     2016 non-null uint8\n",
      "targtype1_19     2016 non-null uint8\n",
      "targtype1_20     2016 non-null uint8\n",
      "targtype1_21     2016 non-null uint8\n",
      "targtype1_22     2016 non-null uint8\n",
      "claimmode_1      2016 non-null uint8\n",
      "claimmode_2      2016 non-null uint8\n",
      "claimmode_3      2016 non-null uint8\n",
      "claimmode_4      2016 non-null uint8\n",
      "claimmode_5      2016 non-null uint8\n",
      "claimmode_6      2016 non-null uint8\n",
      "claimmode_7      2016 non-null uint8\n",
      "claimmode_8      2016 non-null uint8\n",
      "claimmode_9      2016 non-null uint8\n",
      "claimmode_10     2016 non-null uint8\n",
      "dtypes: bool(8), float64(11), uint8(41)\n",
      "memory usage: 285.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done a number of things to prepare our data for modeling. First, we replaced unknown values in each column with the median for that column. Second, we converted attributes that encode a logical value to a boolean. Third, we normalized the ratio attributes so they are all in the range 0 to 1. Fourth, we convert the year, month, and day attribute to a single numeric attribute which represents the day number in the year that the attack occured on. Then we drop the month and day columns. We still want the year attribute because of the change in attack frequency we noticed in Lab 1, so we standardize the year and day number columns. Finally, we one-hot encode all of the categorical attributes in our data set, creating a variety of additional columns that are needed to represent our data in this way.\n",
    "\n",
    "Next, we will split our data set into training and test sets and save our data to a persistant files for easy loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save full clean data set\n",
    "df.to_csv('./clean-data/us-clean.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate train and test sets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "labels = df['propextent']\n",
    "df_attr = df.drop(['propextent'], axis=1)\n",
    "\n",
    "# X_train - 80% training attribute set\n",
    "# X_test - 20% test attribute set\n",
    "# y_train - 80% training labels\n",
    "# y_test - 20% training labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_attr, labels, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=64)\n",
    "\n",
    "X_train.to_csv('./clean-data/train-data.csv', sep=',')\n",
    "X_test.to_csv('./clean-data/test-data.csv', sep=',')\n",
    "y_train.to_csv('./clean-data/train-labels.csv', sep=',')\n",
    "y_test.to_csv('./clean-data/test-labels.csv', sep=',')\n",
    "\n",
    "# need to cross validate to determine if 80-20 split is good for our\n",
    "# data set ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "...\n",
    "\n",
    "First we will load data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2016 entries, 0 to 2756\n",
      "Data columns (total 60 columns):\n",
      "iyear            2016 non-null float64\n",
      "extended         2016 non-null bool\n",
      "vicinity         2016 non-null bool\n",
      "latitude         2016 non-null float64\n",
      "longitude        2016 non-null float64\n",
      "crit1            2016 non-null bool\n",
      "crit2            2016 non-null bool\n",
      "crit3            2016 non-null bool\n",
      "suicide          2016 non-null bool\n",
      "individual       2016 non-null bool\n",
      "nperps           2016 non-null float64\n",
      "nperpcap         2016 non-null float64\n",
      "claimed          2016 non-null bool\n",
      "nkill            2016 non-null float64\n",
      "nkillter         2016 non-null float64\n",
      "nwound           2016 non-null float64\n",
      "nwoundte         2016 non-null float64\n",
      "propextent       2016 non-null float64\n",
      "dayn             2016 non-null float64\n",
      "attacktype1_1    2016 non-null uint8\n",
      "attacktype1_2    2016 non-null uint8\n",
      "attacktype1_3    2016 non-null uint8\n",
      "attacktype1_4    2016 non-null uint8\n",
      "attacktype1_5    2016 non-null uint8\n",
      "attacktype1_6    2016 non-null uint8\n",
      "attacktype1_7    2016 non-null uint8\n",
      "attacktype1_8    2016 non-null uint8\n",
      "attacktype1_9    2016 non-null uint8\n",
      "targtype1_1      2016 non-null uint8\n",
      "targtype1_2      2016 non-null uint8\n",
      "targtype1_3      2016 non-null uint8\n",
      "targtype1_4      2016 non-null uint8\n",
      "targtype1_5      2016 non-null uint8\n",
      "targtype1_6      2016 non-null uint8\n",
      "targtype1_7      2016 non-null uint8\n",
      "targtype1_8      2016 non-null uint8\n",
      "targtype1_9      2016 non-null uint8\n",
      "targtype1_10     2016 non-null uint8\n",
      "targtype1_11     2016 non-null uint8\n",
      "targtype1_12     2016 non-null uint8\n",
      "targtype1_13     2016 non-null uint8\n",
      "targtype1_14     2016 non-null uint8\n",
      "targtype1_15     2016 non-null uint8\n",
      "targtype1_16     2016 non-null uint8\n",
      "targtype1_17     2016 non-null uint8\n",
      "targtype1_18     2016 non-null uint8\n",
      "targtype1_19     2016 non-null uint8\n",
      "targtype1_20     2016 non-null uint8\n",
      "targtype1_21     2016 non-null uint8\n",
      "targtype1_22     2016 non-null uint8\n",
      "claimmode_1      2016 non-null uint8\n",
      "claimmode_2      2016 non-null uint8\n",
      "claimmode_3      2016 non-null uint8\n",
      "claimmode_4      2016 non-null uint8\n",
      "claimmode_5      2016 non-null uint8\n",
      "claimmode_6      2016 non-null uint8\n",
      "claimmode_7      2016 non-null uint8\n",
      "claimmode_8      2016 non-null uint8\n",
      "claimmode_9      2016 non-null uint8\n",
      "claimmode_10     2016 non-null uint8\n",
      "dtypes: bool(8), float64(11), uint8(41)\n",
      "memory usage: 285.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./clean-data/us-clean.csv', index_col=0)\n",
    "\n",
    "# convert one-hot encoded rows to uint8\n",
    "cols = list(df.columns.values)[19:60]\n",
    "df[cols] = df[cols].astype(np.uint8)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('propextent', axis=1).astype(np.float).as_matrix()\n",
    "y = df.as_matrix(columns=['propextent']).astype(np.int).flatten()\n",
    "\n",
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values predicted:  {0, 1, 2}\n",
      "Accuracy of:  0.880456349206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "params = dict(eta=1,\n",
    "              iterations=1500)\n",
    "lr = LogisticRegression(**params)\n",
    "\n",
    "lr.fit(X, y)\n",
    "yhat = lr.predict(X)\n",
    "\n",
    "print(\"Unique values predicted: \" , set(yhat))\n",
    "print('Accuracy of: ', accuracy_score(y,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightedVectorBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    def __init__(self, eta, iterations=20, C=0.001, norm='L1'):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.norm = norm\n",
    "    \n",
    "    def _add_freq_bias(self,X,y):\n",
    "        self.freq_ = np.array(([list(y).count(y[i]) for i in range(len(y))])).reshape(len(y),1)\n",
    "        return np.hstack((self.freq_, X))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_freq_bias(X,y) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient = self._apply_regularization(gradient)\n",
    "        \n",
    "        return gradient\n",
    "\n",
    "    def _apply_regularization(self, gradient):\n",
    "        if self.norm == 'L1':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.norm == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] ** 2 * self.C\n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "            \n",
    "class WeightedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, eta, iterations=20, C=0.001, norm=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.norm = norm\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = WeightedVectorBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values predicted:  {3}\n",
      "Accuracy of:  0.094246031746\n"
     ]
    }
   ],
   "source": [
    "params = dict(eta=0.1,\n",
    "              iterations=1500,\n",
    "              C=0.00001)\n",
    "wlr = WeightedLogisticRegression(**params)\n",
    "wlr.fit(X, y)\n",
    "yhat = wlr.predict(X)\n",
    "\n",
    "print(\"Unique values predicted: \" , set(yhat))\n",
    "print('Accuracy of: ', accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(WeightedVectorBinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient = self._apply_regularization(gradient)\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.0257936507937\n"
     ]
    }
   ],
   "source": [
    "slr = StochasticLogisticRegression(0.1,1500, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.0257936507937\n",
      "CPU times: user 584 ms, sys: 77.6 ms, total: 661 ms\n",
      "Wall time: 500 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(WeightedVectorBinaryLogisticRegression):    \n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "        \n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient = self._apply_regularization(gradient)\n",
    "\n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "hlr = HessianBinaryLogisticRegression(eta=0.1,iterations=10,C=0.01) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X,y)\n",
    "yhat = hlr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(WeightedLogisticRegression):\n",
    "    def __init__(self, classifier_type=0, eta=0.1, iterations=10, C=0.01, norm='L1'):\n",
    "        self.classifier = {'steepest': WeightedVectorBinaryLogisticRegression, \n",
    "                           'stochastic': StochasticLogisticRegression, \n",
    "                           'newton': HessianBinaryLogisticRegression}[classifier_type]\n",
    "        self.params = dict(eta=eta, iterations=iterations, C=C, norm=norm)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = self.classifier(**self.params)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Weighted Vector:  0.877976190476\n",
      "Accuracy of Stochastic:  0.877976190476\n",
      "Accuracy of Hessian:  0.094246031746\n",
      "Accuracy of Weighted Vector w/ L1:  0.877976190476\n",
      "Accuracy of Stochastic w/ L1:  0.877976190476\n",
      "Accuracy of Hessian w/ L1:  0.093253968254\n",
      "Accuracy of Weighted Vector w/ L2:  0.877976190476\n",
      "Accuracy of Stochastic w/ L2:  0.877976190476\n",
      "Accuracy of Hessian w/ L2:  0.094246031746\n"
     ]
    }
   ],
   "source": [
    "# no regularization\n",
    "params = dict(classifier_type='steepest',\n",
    "              eta=0.1,\n",
    "              iterations=500,\n",
    "              C=0.00001,\n",
    "              norm=None)\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Weighted Vector: ',accuracy_score(y,yhat))\n",
    "\n",
    "params = dict(classifier_type='stochastic',\n",
    "              eta=0.1,\n",
    "              iterations=500,\n",
    "              C=0.00001,\n",
    "              norm=None)\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Stochastic: ',accuracy_score(y,yhat))\n",
    "\n",
    "params = dict(classifier_type='newton',\n",
    "              eta=0.1,\n",
    "              iterations=50,\n",
    "              C=0.001,\n",
    "              norm=None)\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Hessian: ',accuracy_score(y,yhat))\n",
    "\n",
    "# L1 norm\n",
    "params = dict(classifier_type='steepest',\n",
    "              eta=0.1,\n",
    "              iterations=500,\n",
    "              C=0.00001)\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Weighted Vector w/ L1: ',accuracy_score(y,yhat))\n",
    "\n",
    "params = dict(classifier_type='stochastic',\n",
    "              eta=0.1,\n",
    "              iterations=500,\n",
    "              C=0.00001)\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Stochastic w/ L1: ',accuracy_score(y,yhat))\n",
    "\n",
    "params = dict(classifier_type='newton',\n",
    "              eta=0.1,\n",
    "              iterations=50,\n",
    "              C=0.001)\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Hessian w/ L1: ',accuracy_score(y,yhat))\n",
    "\n",
    "# L2 norm\n",
    "params = dict(classifier_type='steepest',\n",
    "              eta=0.1,\n",
    "              iterations=500,\n",
    "              C=0.00001,\n",
    "              norm='L2')\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Weighted Vector w/ L2: ',accuracy_score(y,yhat))\n",
    "\n",
    "params = dict(classifier_type='stochastic',\n",
    "              eta=0.1,\n",
    "              iterations=500,\n",
    "              C=0.00001,\n",
    "              norm='L2')\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Stochastic w/ L2: ',accuracy_score(y,yhat))\n",
    "\n",
    "params = dict(classifier_type='newton',\n",
    "              eta=0.1,\n",
    "              iterations=20,\n",
    "              C=0.001,\n",
    "              norm='L2')\n",
    "\n",
    "lrc = LogisticRegressionClassifier(**params)\n",
    "lrc.fit(X, y)\n",
    "yhat = lrc.predict(X)\n",
    "\n",
    "print('Accuracy of Hessian w/ L2: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization Performance\n",
    "Read test and train data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./clean-data/train-data.csv', index_col=0)\n",
    "X_test = pd.read_csv('./clean-data/test-data.csv', index_col=0)\n",
    "y_train = pd.read_csv('./clean-data/train-labels.csv', index_col=0, header=None)\n",
    "y_test = pd.read_csv('./clean-data/test-labels.csv', index_col=0, header=None)\n",
    "\n",
    "X_train = X_train.astype(np.float).as_matrix()\n",
    "X_test = X_test.astype(np.float).as_matrix()\n",
    "y_train = y_train.as_matrix().astype(np.int).flatten()\n",
    "y_test = y_test.as_matrix().astype(np.int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1612, 59)\n",
      "(1612,)\n",
      "(404, 59)\n",
      "(404,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 6, 3)\n"
     ]
    }
   ],
   "source": [
    "n_iterations = [10, 250, 500, 750, 1000, 1250, 1500]\n",
    "Cs = [0.0001, 0.01, 0.1, 0.5, 0.8, 1]\n",
    "types = ['steepest', 'stochastic', 'newton']\n",
    "print((len(n_iterations), len(Cs), len(types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = np.zeros((len(n_iterations), len(Cs), len(types)))\n",
    "\n",
    "for n in range(len(n_iterations)):\n",
    "    for c in range(len(Cs)):\n",
    "        for t in range(len(types)):\n",
    "            params = dict(classifier_type=types[t],\n",
    "                          eta=0.1,\n",
    "                          iterations=n_iterations[n],\n",
    "                          C=Cs[c],\n",
    "                          norm=None)\n",
    "            lrc = LogisticRegressionClassifier(**params)\n",
    "            lrc.fit(X_train, y_train)\n",
    "            yhat = lrc.predict(X_test)\n",
    "            accuracies[n][c][t] = accuracy_score(y_test, yhat)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891]],\n",
       "\n",
       "       [[ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891],\n",
       "        [ 0.87376238,  0.87376238,  0.08910891]],\n",
       "\n",
       "       [[ 0.87376238,  0.87376238,  0.03712871],\n",
       "        [ 0.87376238,  0.87376238,  0.03712871],\n",
       "        [ 0.87376238,  0.57673267,  0.03712871],\n",
       "        [ 0.87376238,  0.87376238,  0.03465347],\n",
       "        [ 0.87376238,  0.09158416,  0.07425743],\n",
       "        [ 0.87376238,  0.87376238,  0.0990099 ]],\n",
       "\n",
       "       [[ 0.87376238,  0.87376238,  0.03712871],\n",
       "        [ 0.87376238,  0.87376238,  0.03712871],\n",
       "        [ 0.87376238,  0.87376238,  0.03712871],\n",
       "        [ 0.87376238,  0.87376238,  0.03465347],\n",
       "        [ 0.87376238,  0.87376238,  0.07425743],\n",
       "        [ 0.87376238,  0.09158416,  0.0990099 ]],\n",
       "\n",
       "       [[ 0.87376238,  0.08910891,  0.03712871],\n",
       "        [ 0.87376238,  0.18811881,  0.03712871],\n",
       "        [ 0.87376238,  0.87376238,  0.03712871],\n",
       "        [ 0.87376238,  0.08910891,  0.03465347],\n",
       "        [ 0.87376238,  0.87376238,  0.07425743],\n",
       "        [ 0.87376238,  0.44306931,  0.0990099 ]],\n",
       "\n",
       "       [[ 0.08910891,  0.12623762,  0.03712871],\n",
       "        [ 0.08910891,  0.09158416,  0.03712871],\n",
       "        [ 0.08910891,  0.08910891,  0.03712871],\n",
       "        [ 0.08910891,  0.87376238,  0.03465347],\n",
       "        [ 0.08910891,  0.08910891,  0.07425743],\n",
       "        [ 0.08910891,  0.16584158,  0.0990099 ]],\n",
       "\n",
       "       [[ 0.09405941,  0.63118812,  0.03712871],\n",
       "        [ 0.09405941,  0.08910891,  0.03712871],\n",
       "        [ 0.09405941,  0.08910891,  0.03712871],\n",
       "        [ 0.09405941,  0.09158416,  0.03465347],\n",
       "        [ 0.09405941,  0.08910891,  0.07425743],\n",
       "        [ 0.09405941,  0.08910891,  0.0990099 ]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.savetxt('./clean-data/accuracies.csv', accuracies, delimiter=',')\n",
    "import json\n",
    "with open('./clean-data/accuracies.json', 'w') as f: f.write(json.dumps(accuracies, default=lambda x: list(x), indent=4))\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-221db0b7f870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./clean-data/accuracies.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# accuracies = accuracies.reshape((len(n_iterations), len(Cs), len(types)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/MachineLearning/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/MachineLearning/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/MachineLearning/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "accuracies = np.array(json.loads('./clean-data/accuracies.json'))\n",
    "print(accuracies)\n",
    "# accuracies = accuracies.reshape((len(n_iterations), len(Cs), len(types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "Federal Grant Policy:\n",
    "<a href=\"https://www.grants.gov/web/grants/learn-grants/grant-policies.html\">A Short History of the Federal Grant Policy</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MachineLearning]",
   "language": "python",
   "name": "conda-env-MachineLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
