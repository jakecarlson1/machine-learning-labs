{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Wide and Deep Networks\n",
    "\n",
    "### Eric Smith and Jake Carlson\n",
    "\n",
    "## Introduction\n",
    "For this lab, we will again be examining the Global Terrorism Database maintained by the National Consortium for the Study of Terrorism and Responses to Terrorism (START) at the University of Maryland. We will be looking at attacks that happened in the United States over the whole time span of the data set, since it's creation in 1974.\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "### Motivations\n",
    "Protecting the United States from terror threats has been a major objective of the federal government. This is characterized by the founding of the Department of Homeland Security in 2001. But predicting when an attack will happen based on certain attributes is next to impossible. Attempting to train a model on the Global Terrorism Database to learn when terrorist attacks happen will result in a model that is over-trained on the GTD and will fail to predict any such attacks. Not to mention, such a system would have to be accompanied by a large-scale communication monitoring and processing system capable of feeding the model relevant inputs that exemplify a possible attack.\n",
    "\n",
    "Instead of trying to predict when an attack will happen, our goal is to create a model that can predict the cost associate with an individual attack. Immediately after an attack has happened, law enforcement can feed in information about the attack, such as the attack type, the number of people injured, and the target type, and they could receive an approximation of the amount of property damage dealt to their city. Such a model would allow city officials and law enforcement to estimate in real time how much an attack will cost their city. Knowing the estimated cost would enable city officials to determine if they need to request support from the federal government in a shorter timeframe. Furthermore, cities could plan their future budgets accordingly to incorporate funding in response to a terrorist attack.\n",
    "\n",
    "Cities have to submit requests to FEMA for non-disaster grants to aid in the prevention and response to terrorist activity. The Department of Homeland Security can also issue grants to aid in the prevention of terrorism. Grant policies start with Congress allocating funds for federal grants of this type. The Executive Branch provides input for how the policy should be implemented. Then grant issuing agencies develop their own policies for how to allocate grant money.\n",
    "\n",
    "Each state defines their own thresholds for when an attack is severe enough that they will ask for federal assistance. Our model will allow officials to immediately decide if they need to file for a federal grant. Smaller cities have lwoer thresholds and larger cities can handle higher costs before needing assistance.\n",
    "\n",
    "### Objectives\n",
    "Based on the characteristics of an attack, such as the target type and the date, we want to assign an estimated cost label to the entity. Because our system will be used to estimate the cost for local city governements, perfect classification of cost is not required. However, it is important that these estimations are accurate because a request for a grant will need to be formed and sent to the federal government.\n",
    "\n",
    "Based on the distribution of our classes we want to achieve an accuracy that is greater than the ratio of the majority class to the rest of the population. The class counts are given by:\n",
    "\n",
    "    Catastrophic (class 0): 4\n",
    "    Major (class 1): 52\n",
    "    Minor (class 2): 1770\n",
    "    Unknown (class 3): 190\n",
    "\n",
    "The majority class is class 2, which constitutes 87% of the data set. We want to achieve a classification accuracy greater than this for our model to be useful.\n",
    "\n",
    "## REDO everything above this**\n",
    "\n",
    "### Evaluation\n",
    "Because we are predicting the group that conducted the attack, it would be an issue if we predicted the wrong group. Law enforcement could waste time and resources following the incorrect prediction and the perpetrators would have more time to get away or plan another attack. We will evaluate our model using the precision score in order to minimize the false positive rate. We will use macro precision so all of the groups are weighted equally.\n",
    "\n",
    "Because some of the groups are over-represented, we will use stratified 10-fold cross validation so the classes in each fold match the distribution of the original data set. Running training and testing ten times will also allow us to be confident in the generalization performance of the model.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Attributes\n",
    "Here is the list of attributes we will keep in our data set to use for classification.\n",
    "\n",
    "#### General Information\n",
    "- **iyear** (ordinal): The year the event occured in\n",
    "- **imonth** (ordinal): The month the event occured in\n",
    "- **iday** (ordinal): The day the event occured in\n",
    "- **extended** (binary): 1 if the incident was longer than 24 hours, 0 otherwise\n",
    "    - **resolution** (ordinal): The date an extended incident was resolved if *extended* is 1\n",
    "\n",
    "\n",
    "- **inclusion criteria** (binary): There are three inclusion criteria where a 1 indicates the event meets that criteria\n",
    "    - **crit1**: Political, economic, religious, or social goal\n",
    "    - **crit2**: Intention to coerce, intimidate, or publicize\n",
    "    - **crit3**: Outside international humanitarian law\n",
    "\n",
    "\n",
    "#### Location\n",
    "We will provide the name of the city to the model. An alternative method would be to train a unique logistic regression algorithm for each city where our system is deployed.\n",
    "- **city** (text): Name of the city in which the event occured\n",
    "- **vicinity** (nominal/binary): A 1 indicates the event occured in the immediate vicinity of *city*, 0 indicates the even occured in *city*\n",
    "- **latitude** (ratio): The latitude of the *city* in which the event occured\n",
    "- **longitude** (ratio): The longitude of the *city* in which the event occured\n",
    "\n",
    "#### Attack Type\n",
    "The most severe method of attack. This will be our class label. Although the original data set contains columns for three different attack types, the attack types are ranked by their severity. Many attacks only have one attack type. By removing the second and third attack types from our data set, we will still be predicting the most severe of the attack types.\n",
    "- **attacktype1** (ordinal): Most severe attack type\n",
    "\n",
    "- The attack types follow the following hierarchy:\n",
    "    1. Assassination\n",
    "    2. Armed Assault\n",
    "    3. Bombing/Explosion\n",
    "    4. Hijacking\n",
    "    5. Barricade Incident\n",
    "    6. Kidnapping\n",
    "    7. Facility/Infrastructure Attack \n",
    "    8. Unarmed Assault\n",
    "    9. Unknown\n",
    "\n",
    "\n",
    "- **suicide** (nominal/binary): A 1 indicates there was evidence the attacker did not make an effort to escape with their life\n",
    "\n",
    "#### Target Type\n",
    "We will only be considering the first target type of the attack. The set of target attributes is provided below:\n",
    "- **targtype1, targtype1_txt** (nominal): The general type of target from the following list:\n",
    "    1. Business\n",
    "    2. Government (General)\n",
    "    3. Police\n",
    "    4. Military\n",
    "    5. Abortion related\n",
    "    6. Airports and aircraft\n",
    "    7. Government (Diplomatic)\n",
    "    8. Educational institution\n",
    "    9. Food or water supply\n",
    "    10. Journalists and media\n",
    "    11. Maritime\n",
    "    12. NGO\n",
    "    13. Other\n",
    "    14. Private citizens and property\n",
    "    15. Religious figures and institutions\n",
    "    16. Telecommunication\n",
    "    17. Terrirists and non-state militias\n",
    "    18. Tourists\n",
    "    19. Transportation\n",
    "    20. Unknown\n",
    "    21. Utilities\n",
    "    22. Violent political parties\n",
    "    \n",
    "\n",
    "- **targsubtype1, targsubtype1_txt** (nominal): There are a number of subtypes for each of the above target types\n",
    "\n",
    "#### Perpetrator Information\n",
    "The data set provides information on up to three perpetrators if the attack was conducted by multiple groups. We will only be considering the first group, or the one decided to have the most responsibility for the attack.\n",
    "- **individual** (binary): A 1 indicates the individuals carrying out the attack are not affiliated with a terror organization\n",
    "- **nperps** (ratio): Indicates the total number of terrorists participating in the event\n",
    "- **nperpcap** (ratio): Number of perpatrators taken into custody\n",
    "- **claimed** (binary): A 1 indicates a person or group claimed responsibility for the attack\n",
    "- **claimmode** (nominal): Records the method the terror group used to claim responsibility for the attack. Can be one of the ten following categories:\n",
    "    1. Letter\n",
    "    2. Call (post-incident)\n",
    "    3. Call (pre-incident)\n",
    "    4. E-mail\n",
    "    5. Note left at scene\n",
    "    6. Video\n",
    "    7. Posted to website\n",
    "    8. Personal claim\n",
    "    9. Other\n",
    "    10. Unknown\n",
    "\n",
    "\n",
    "#### Casualties and Consequences\n",
    "- **nkill** (ratio): Records the number of confirmed kills for the incident\n",
    "- **nkillter** (ratio): Indicates the number of terrorists who were killed in the event\n",
    "- **nwound** (ratio): Indicates the number of people who sustained non-fatal injuries in the event\n",
    "- **nwoundte** (ratio): Indicates the number of terrorists who sustained non-lethal injuries\n",
    "- **property** (binary): A 1 indicates the event resulted in property damage. We will only select entities that resulted in property damage\n",
    "- **propextent** (ordinal): If *property* is a 1, this field records the extent of the property damage following the scheme:\n",
    "    <ol start='0'>\n",
    "        <li>Catastrophic (likely > \\$1 billion)</li>\n",
    "        <li>Major (likely > \\$1 million and < \\$1 billion)</li>\n",
    "        <li>Minor (likely < \\$1 million)</li>\n",
    "        <li>Unknown</li>\n",
    "    </ol>\n",
    "\n",
    "### Data Cleaning\n",
    "We will clean the data set so only the above attributes are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventid</th>\n",
       "      <th>iyear</th>\n",
       "      <th>imonth</th>\n",
       "      <th>iday</th>\n",
       "      <th>approxdate</th>\n",
       "      <th>extended</th>\n",
       "      <th>resolution</th>\n",
       "      <th>country</th>\n",
       "      <th>country_txt</th>\n",
       "      <th>region</th>\n",
       "      <th>...</th>\n",
       "      <th>addnotes</th>\n",
       "      <th>scite1</th>\n",
       "      <th>scite2</th>\n",
       "      <th>scite3</th>\n",
       "      <th>dbsource</th>\n",
       "      <th>INT_LOG</th>\n",
       "      <th>INT_IDEO</th>\n",
       "      <th>INT_MISC</th>\n",
       "      <th>INT_ANY</th>\n",
       "      <th>related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>At least 10 army soldiers were killed by Front...</td>\n",
       "      <td>\"Burundi: Rebels Ambush Minibus North of Bujum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CETIS</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229</td>\n",
       "      <td>Democratic Republic of the Congo</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>The soldiers arrived at the location of the at...</td>\n",
       "      <td>\"DRCongo: Four Killed in Shooting in Rebel Hel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CETIS</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>Israel</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Israeli investigators believed the attack was ...</td>\n",
       "      <td>\"Two Border Policemen Killed, One Wounded in a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CETIS</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>This attack was one of four related incidents ...</td>\n",
       "      <td>United States Government, The 9/11 Commission ...</td>\n",
       "      <td>Lindsay Kines, ñUnited States on high alert af...</td>\n",
       "      <td>Joe Frolick, ñHijackers Ram Two Airliners Into...</td>\n",
       "      <td>CETIS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200109110004, 200109110005, 200109110006, 2001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>This attack was one of four related incidents ...</td>\n",
       "      <td>United States Government, The 9/11 Commission ...</td>\n",
       "      <td>Lindsay Kines, ñUnited States on high alert af...</td>\n",
       "      <td>Joe Frolick, ñHijackers Ram Two Airliners Into...</td>\n",
       "      <td>CETIS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200109110005, 200109110004, 200109110006, 2001...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        eventid  iyear  imonth  iday approxdate  extended resolution  country  \\\n",
       "0  2.000000e+11   2001       9    11        NaN         0        NaN       34   \n",
       "1  2.000000e+11   2001       9    11        NaN         0        NaN      229   \n",
       "2  2.000000e+11   2001       9    11        NaN         0        NaN       97   \n",
       "3  2.000000e+11   2001       9    11        NaN         0        NaN      217   \n",
       "4  2.000000e+11   2001       9    11        NaN         0        NaN      217   \n",
       "\n",
       "                        country_txt  region  \\\n",
       "0                           Burundi      11   \n",
       "1  Democratic Republic of the Congo      11   \n",
       "2                            Israel      10   \n",
       "3                     United States       1   \n",
       "4                     United States       1   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                            addnotes  \\\n",
       "0  At least 10 army soldiers were killed by Front...   \n",
       "1  The soldiers arrived at the location of the at...   \n",
       "2  Israeli investigators believed the attack was ...   \n",
       "3  This attack was one of four related incidents ...   \n",
       "4  This attack was one of four related incidents ...   \n",
       "\n",
       "                                              scite1  \\\n",
       "0  \"Burundi: Rebels Ambush Minibus North of Bujum...   \n",
       "1  \"DRCongo: Four Killed in Shooting in Rebel Hel...   \n",
       "2  \"Two Border Policemen Killed, One Wounded in a...   \n",
       "3  United States Government, The 9/11 Commission ...   \n",
       "4  United States Government, The 9/11 Commission ...   \n",
       "\n",
       "                                              scite2  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Lindsay Kines, ñUnited States on high alert af...   \n",
       "4  Lindsay Kines, ñUnited States on high alert af...   \n",
       "\n",
       "                                              scite3  dbsource  INT_LOG  \\\n",
       "0                                                NaN     CETIS       -9   \n",
       "1                                                NaN     CETIS       -9   \n",
       "2                                                NaN     CETIS       -9   \n",
       "3  Joe Frolick, ñHijackers Ram Two Airliners Into...     CETIS        0   \n",
       "4  Joe Frolick, ñHijackers Ram Two Airliners Into...     CETIS        0   \n",
       "\n",
       "   INT_IDEO INT_MISC INT_ANY  \\\n",
       "0        -9        0      -9   \n",
       "1        -9        0      -9   \n",
       "2        -9        0      -9   \n",
       "3         1        0       1   \n",
       "4         1        0       1   \n",
       "\n",
       "                                             related  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  200109110004, 200109110005, 200109110006, 2001...  \n",
       "4  200109110005, 200109110004, 200109110006, 2001...  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/After_911.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['eventid', 'iyear', 'imonth', 'iday', 'approxdate', 'extended',\n",
       "       'resolution', 'country', 'country_txt', 'region', 'region_txt',\n",
       "       'provstate', 'city', 'latitude', 'longitude', 'specificity',\n",
       "       'vicinity', 'location', 'summary', 'crit1', 'crit2', 'crit3',\n",
       "       'doubtterr', 'alternative', 'alternative_txt', 'multiple',\n",
       "       'success', 'suicide', 'attacktype1', 'attacktype1_txt',\n",
       "       'attacktype2', 'attacktype2_txt', 'attacktype3', 'attacktype3_txt',\n",
       "       'targtype1', 'targtype1_txt', 'targsubtype1', 'targsubtype1_txt',\n",
       "       'corp1', 'target1', 'natlty1', 'natlty1_txt', 'targtype2',\n",
       "       'targtype2_txt', 'targsubtype2', 'targsubtype2_txt', 'corp2',\n",
       "       'target2', 'natlty2', 'natlty2_txt', 'targtype3', 'targtype3_txt',\n",
       "       'targsubtype3', 'targsubtype3_txt', 'corp3', 'target3', 'natlty3',\n",
       "       'natlty3_txt', 'gname', 'gsubname', 'gname2', 'gsubname2', 'gname3',\n",
       "       'gsubname3', 'motive', 'guncertain1', 'guncertain2', 'guncertain3',\n",
       "       'individual', 'nperps', 'nperpcap', 'claimed', 'claimmode',\n",
       "       'claimmode_txt', 'claim2', 'claimmode2', 'claimmode2_txt', 'claim3',\n",
       "       'claimmode3', 'claimmode3_txt', 'compclaim', 'weaptype1',\n",
       "       'weaptype1_txt', 'weapsubtype1', 'weapsubtype1_txt', 'weaptype2',\n",
       "       'weaptype2_txt', 'weapsubtype2', 'weapsubtype2_txt', 'weaptype3',\n",
       "       'weaptype3_txt', 'weapsubtype3', 'weapsubtype3_txt', 'weaptype4',\n",
       "       'weaptype4_txt', 'weapsubtype4', 'weapsubtype4_txt', 'weapdetail',\n",
       "       'nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', 'nwoundte',\n",
       "       'property', 'propextent', 'propextent_txt', 'propvalue',\n",
       "       'propcomment', 'ishostkid', 'nhostkid', 'nhostkidus', 'nhours',\n",
       "       'ndays', 'divert', 'kidhijcountry', 'ransom', 'ransomamt',\n",
       "       'ransomamtus', 'ransompaid', 'ransompaidus', 'ransomnote',\n",
       "       'hostkidoutcome', 'hostkidoutcome_txt', 'nreleased', 'addnotes',\n",
       "       'scite1', 'scite2', 'scite3', 'dbsource', 'INT_LOG', 'INT_IDEO',\n",
       "       'INT_MISC', 'INT_ANY', 'related'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_keep = ['eventid', 'extended', 'iyear', 'imonth', 'approxdate', 'iday', 'crit1', 'crit2',\n",
    "           'crit3', 'country', 'city', 'vicinity', 'latitude', 'longitude',\n",
    "           'attacktype1_txt', 'attacktype2_txt',\n",
    "           'attacktype3_txt', 'success', 'suicide',\n",
    "           'targtype1_txt', 'gname', 'individual', \n",
    "           'nperps', 'nperpcap', 'claimed', 'nkill', 'nkillter', 'nwound', 'nwoundte',\n",
    "           'property', 'propextent', 'propextent_txt', 'propvalue',\n",
    "           'ishostkid', 'nhostkid', 'nreleased']\n",
    "df = df[to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventid</th>\n",
       "      <th>extended</th>\n",
       "      <th>iyear</th>\n",
       "      <th>approxdate</th>\n",
       "      <th>crit1</th>\n",
       "      <th>crit2</th>\n",
       "      <th>crit3</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>...</th>\n",
       "      <th>nwound</th>\n",
       "      <th>nwoundte</th>\n",
       "      <th>property</th>\n",
       "      <th>propextent</th>\n",
       "      <th>propextent_txt</th>\n",
       "      <th>propvalue</th>\n",
       "      <th>ishostkid</th>\n",
       "      <th>nhostkid</th>\n",
       "      <th>nreleased</th>\n",
       "      <th>dayn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>217</td>\n",
       "      <td>New York City</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Catastrophic (likely &gt; $1 billion)</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>217</td>\n",
       "      <td>New York City</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Catastrophic (likely &gt; $1 billion)</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>217</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Catastrophic (likely &gt; $1 billion)</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>217</td>\n",
       "      <td>Shanksville</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Catastrophic (likely &gt; $1 billion)</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.000000e+11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>160</td>\n",
       "      <td>Barira</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        eventid  extended  iyear approxdate  crit1  crit2  crit3  country  \\\n",
       "3  2.000000e+11     False    0.0        NaN   True   True   True      217   \n",
       "4  2.000000e+11     False    0.0        NaN   True   True   True      217   \n",
       "5  2.000000e+11     False    0.0        NaN   True   True   True      217   \n",
       "6  2.000000e+11     False    0.0        NaN   True   True   True      217   \n",
       "9  2.000000e+11     False    0.0        NaN   True   True   True      160   \n",
       "\n",
       "            city  vicinity    ...       nwound  nwoundte property propextent  \\\n",
       "3  New York City     False    ...     1.000000       0.0     True        1.0   \n",
       "4  New York City     False    ...     0.999864       0.0     True        1.0   \n",
       "5      Arlington     False    ...     0.014390       0.0     True        1.0   \n",
       "6    Shanksville      True    ...     0.000679       0.0     True        1.0   \n",
       "9         Barira     False    ...     0.000815       0.0    False        NaN   \n",
       "\n",
       "                       propextent_txt  propvalue  ishostkid nhostkid  \\\n",
       "3  Catastrophic (likely > $1 billion)   0.002911        1.0     88.0   \n",
       "4  Catastrophic (likely > $1 billion)   0.002911        1.0     59.0   \n",
       "5  Catastrophic (likely > $1 billion)   0.002911        1.0     59.0   \n",
       "6  Catastrophic (likely > $1 billion)   0.002911        1.0     40.0   \n",
       "9                                 NaN   0.002911        0.0      0.0   \n",
       "\n",
       "  nreleased      dayn  \n",
       "3       0.0  0.081967  \n",
       "4       0.0  0.081967  \n",
       "5       0.0  0.081967  \n",
       "6       0.0  0.081967  \n",
       "9       NaN  0.098361  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "\n",
    "logical_cols = ['extended', 'vicinity', 'crit1', 'crit2', 'crit3',\n",
    "                'suicide', 'individual', 'claimed', 'success', 'property']\n",
    "categorical_cols = ['attacktype1_txt', 'attacktype2_txt', 'attacktype3_txt', \n",
    "                    'targtype1_txt', 'country', 'city', 'gname', 'propextent_txt']\n",
    "ratio_cols = ['latitude', 'longitude', 'nperps', 'nperpcap', 'nkill',\n",
    "              'nkillter', 'nwound', 'nwoundte', 'propvalue', 'ishostkid']\n",
    "\n",
    "# replace unknowns with nan\n",
    "logical_replace = dict((l, {-9:np.nan}) for l in logical_cols)\n",
    "ratio_replace = dict((r, {-99:np.nan, -9:np.nan}) for r in ratio_cols)\n",
    "df.replace(to_replace=logical_replace, inplace=True)\n",
    "df.replace(to_replace=ratio_replace, inplace=True)\n",
    "\n",
    "# replace unknowns with median\n",
    "logical_replace = dict((l, {np.nan:df[l].median()}) for l in logical_cols)\n",
    "ratio_replace = dict((r, {np.nan:df[r].median()}) for r in ratio_cols)\n",
    "df.replace(to_replace=logical_replace, inplace=True)\n",
    "df.replace(to_replace=ratio_replace, inplace=True)\n",
    "\n",
    "# impute nhostkid column\n",
    "for index, row in df.iterrows():\n",
    "    if row.ishostkid == 0:\n",
    "        df.loc[index, 'nhostkid'] = 0\n",
    "\n",
    "# convert logical cols to bools\n",
    "for l in logical_cols:\n",
    "    df[l] = df[l].astype('bool')\n",
    "\n",
    "# replace dates with the first date of the approximate range\n",
    "for index, row in df[ df.approxdate.notnull() ].iterrows():\n",
    "    date = dateutil.parser.parse( row.approxdate.split('-')[0] )\n",
    "    df.loc[index, 'imonth'] = date.month\n",
    "    df.loc[index, 'iday'] = date.day\n",
    "\n",
    "\n",
    "# normalize ratio cols\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df[ratio_cols] = min_max_scaler.fit_transform(df[ratio_cols])\n",
    "\n",
    "# standardize date attributes\n",
    "# use year, month, and day to get day number in year\n",
    "day_list = []\n",
    "for r in df[['iyear', 'imonth', 'iday']].iterrows():\n",
    "    # fudge day 0 to 1\n",
    "    if r[1].iday == 0:\n",
    "        day_list.append(\n",
    "            datetime(r[1].iyear, r[1].imonth, 1).timetuple().tm_yday)\n",
    "    else:\n",
    "        day_list.append(\n",
    "            datetime(r[1].iyear, r[1].imonth, r[1].iday).timetuple().tm_yday)\n",
    "        \n",
    "df = df.assign(dayn=day_list)\n",
    "\n",
    "# drop month and day attributes\n",
    "df.drop(['imonth', 'iday'], axis=1, inplace=True)\n",
    "\n",
    "# normalize day number and year col\n",
    "df['iyear'] = df['iyear'].astype(np.float64)\n",
    "df['dayn'] = df['dayn'].astype(np.float64)\n",
    "df[['iyear', 'dayn']] = min_max_scaler.fit_transform(df[['iyear', 'dayn']])\n",
    "\n",
    "# drop unknown groups\n",
    "df = df[df.gname != \"Unknown\"]\n",
    "\n",
    "# one-hot encode categorical cols\n",
    "# df = pd.get_dummies(df, prefix=categorical_cols, columns=categorical_cols)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save full clean data set\n",
    "df.to_csv('./clean-data/After_911_clean.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done a number of things to prepare our data for modeling. First, we replaced unknown values in each column with the median for that column. Second, we converted attributes that encode a logical value to a boolean. Third, we normalized the ratio attributes so they are all in the range 0 to 1. Fourth, we convert the year, month, and day attribute to a single numeric attribute which represents the day number in the year that the attack occured on. Then we drop the month and day columns. We still want the year attribute because of the change in attack frequency we noticed in Lab 1, so we standardize the year and day number columns. Finally, we one-hot encode all of the categorical attributes in our data set, creating a variety of additional columns that are needed to represent our data in this way.\n",
    "\n",
    "### Crossed Columns\n",
    "We will create several crossed columns to make the data set wider. This will help our model with memorization of the training data. We will cross attack type with property extent, target type, city, and country. We will also cross property extent with target type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./clean-data/After_911_clean.csv')\n",
    "\n",
    "y = df['propextent']\n",
    "X = df.drop(['propextent'], axis=1)\n",
    "# y_ints, y_levels = pd.factorize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 1.0,\n",
       " 2.0,\n",
       " nan,\n",
       " nan,\n",
       " 4.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JakeCarlson/anaconda/envs/MLEnv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (6,63,79,94,96,114,115) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent maintained:  36.354092102123595 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iyear</th>\n",
       "      <th>extended</th>\n",
       "      <th>vicinity</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>crit1</th>\n",
       "      <th>crit2</th>\n",
       "      <th>crit3</th>\n",
       "      <th>suicide</th>\n",
       "      <th>individual</th>\n",
       "      <th>...</th>\n",
       "      <th>claimmode_1.0</th>\n",
       "      <th>claimmode_2.0</th>\n",
       "      <th>claimmode_3.0</th>\n",
       "      <th>claimmode_4.0</th>\n",
       "      <th>claimmode_5.0</th>\n",
       "      <th>claimmode_6.0</th>\n",
       "      <th>claimmode_7.0</th>\n",
       "      <th>claimmode_8.0</th>\n",
       "      <th>claimmode_9.0</th>\n",
       "      <th>claimmode_10.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.758239</td>\n",
       "      <td>0.161304</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.758239</td>\n",
       "      <td>0.161304</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.740602</td>\n",
       "      <td>0.150956</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.751555</td>\n",
       "      <td>0.144956</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.733236</td>\n",
       "      <td>0.452702</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3433 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iyear  extended  vicinity  latitude  longitude  crit1  crit2  crit3  \\\n",
       "3     0.0     False     False  0.758239   0.161304   True   True   True   \n",
       "4     0.0     False     False  0.758239   0.161304   True   True   True   \n",
       "5     0.0     False     False  0.740602   0.150956   True   True   True   \n",
       "6     0.0     False      True  0.751555   0.144956   True   True   True   \n",
       "10    0.0     False     False  0.733236   0.452702   True   True   True   \n",
       "\n",
       "    suicide  individual       ...        claimmode_1.0  claimmode_2.0  \\\n",
       "3      True       False       ...                    0              0   \n",
       "4      True       False       ...                    0              0   \n",
       "5      True       False       ...                    0              0   \n",
       "6      True       False       ...                    0              0   \n",
       "10    False       False       ...                    0              0   \n",
       "\n",
       "    claimmode_3.0  claimmode_4.0  claimmode_5.0  claimmode_6.0  claimmode_7.0  \\\n",
       "3               0              0              0              1              0   \n",
       "4               0              0              0              1              0   \n",
       "5               0              0              0              1              0   \n",
       "6               0              0              0              1              0   \n",
       "10              0              0              0              0              1   \n",
       "\n",
       "    claimmode_8.0  claimmode_9.0  claimmode_10.0  \n",
       "3               0              0               0  \n",
       "4               0              0               0  \n",
       "5               0              0               0  \n",
       "6               0              0               0  \n",
       "10              0              0               0  \n",
       "\n",
       "[5 rows x 3433 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/After_911.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# drop rows without property damage or unknown city\n",
    "orig_len = df.shape[0]\n",
    "df = df[df['property'] == 1]\n",
    "df = df[df['city'] != \"Unknown\"]\n",
    "new_len = df.shape[0]\n",
    "print(\"Percent maintained: \", new_len/orig_len*100, \"%\")\n",
    "\n",
    "# select columns of interest\n",
    "df = df[['iyear', 'imonth', 'iday', 'extended', 'country', 'city', 'vicinity',\n",
    "         'latitude', 'longitude', 'crit1', 'crit2', 'crit3', 'suicide',\n",
    "         'attacktype1_txt', 'targtype1_txt', 'individual', 'nperps', 'nperpcap', \n",
    "         'claimed', 'claimmode', 'nkill', 'nkillter', 'nwound', \n",
    "         'nwoundte', 'propextent']]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "logical_cols = ['extended', 'vicinity', 'crit1', 'crit2', 'crit3',\n",
    "                'suicide', 'individual', 'claimed']\n",
    "categorical_cols = ['attacktype1_txt', 'targtype1_txt', 'country', 'city', 'claimmode']\n",
    "ratio_cols = ['latitude', 'longitude', 'nperps', 'nperpcap', 'nkill',\n",
    "              'nkillter', 'nwound', 'nwoundte']\n",
    "\n",
    "# replace unknowns with nan\n",
    "logical_replace = dict((l, {-9:np.nan}) for l in ['claimed'])\n",
    "ratio_replace = dict((r, {-99:np.nan, -9:np.nan}) for r in ratio_cols)\n",
    "df.replace(to_replace=logical_replace, inplace=True)\n",
    "df.replace(to_replace=ratio_replace, inplace=True)\n",
    "\n",
    "# might want to remove outliers here\n",
    "\n",
    "# replace NA's with median for col\n",
    "df.fillna(value=df.median(), inplace=True)\n",
    "\n",
    "# convert logical cols to bools\n",
    "for l in logical_cols:\n",
    "    df[l] = df[l].astype('bool')\n",
    "\n",
    "# normalize ratio cols\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df[ratio_cols] = min_max_scaler.fit_transform(df[ratio_cols])\n",
    "\n",
    "# standardize date attributes\n",
    "# use year, month, and day to get day number in year\n",
    "day_list = []\n",
    "for r in df[['iyear', 'imonth', 'iday']].iterrows():\n",
    "    # fudge day 0 to 1\n",
    "    if r[1].iday == 0:\n",
    "        day_list.append(\n",
    "            datetime(r[1].iyear, r[1].imonth, 1).timetuple().tm_yday)\n",
    "    else:\n",
    "        day_list.append(\n",
    "            datetime(r[1].iyear, r[1].imonth, r[1].iday).timetuple().tm_yday)\n",
    "        \n",
    "df = df.assign(dayn=day_list)\n",
    "\n",
    "# drop month and day attributes\n",
    "df.drop(['imonth', 'iday'], axis=1, inplace=True)\n",
    "\n",
    "# normalize day number and year col\n",
    "df['iyear'] = df['iyear'].astype(np.float64)\n",
    "df['dayn'] = df['dayn'].astype(np.float64)\n",
    "df[['iyear', 'dayn']] = min_max_scaler.fit_transform(df[['iyear', 'dayn']])\n",
    "\n",
    "# convert categorical cols as ints\n",
    "# for c in categorical_cols:\n",
    "#     df[c] = df[c].astype(np.int64)\n",
    "\n",
    "# one-hot encode categorical cols\n",
    "df = pd.get_dummies(df, prefix=categorical_cols, columns=categorical_cols)\n",
    "\n",
    "# zero-index propextent\n",
    "df.propextent = df.propextent - 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['propextent']\n",
    "X = df.drop(['propextent'], axis=1)\n",
    "y_ints = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7618/7618 [==============================] - 2s - loss: 2.4907 - acc: 0.0116     \n",
      "Epoch 2/10\n",
      "7618/7618 [==============================] - 2s - loss: 1.9770 - acc: 0.0116     \n",
      "Epoch 3/10\n",
      "7618/7618 [==============================] - 2s - loss: 1.8365 - acc: 0.0116     \n",
      "Epoch 4/10\n",
      "7618/7618 [==============================] - 2s - loss: 1.7769 - acc: 0.0116     \n",
      "Epoch 5/10\n",
      "7618/7618 [==============================] - 3s - loss: 1.7446 - acc: 0.0116     \n",
      "Epoch 6/10\n",
      "7618/7618 [==============================] - 2s - loss: 1.7245 - acc: 0.0116     \n",
      "Epoch 7/10\n",
      "7618/7618 [==============================] - 2s - loss: 1.7108 - acc: 0.0116     \n",
      "Epoch 8/10\n",
      "7618/7618 [==============================] - 2s - loss: 1.7008 - acc: 0.0116     \n",
      "Epoch 9/10\n",
      "7618/7618 [==============================] - 3s - loss: 1.6933 - acc: 0.0116     \n",
      "Epoch 10/10\n",
      "7618/7618 [==============================] - 2s - loss: 1.6875 - acc: 0.0116     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137741668>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Merge, concatenate\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cross_columns = [['attacktype1_txt','targtype1_txt'],\n",
    "                 ['attacktype1_txt','city'],\n",
    "                 ['attacktype1_txt','country']]\n",
    "\n",
    "X_ints = []\n",
    "all_inputs = []\n",
    "all_branch_outputs = []\n",
    "\n",
    "for cols in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # create crossed labels\n",
    "    X_crossed = df[cols].apply(lambda x: '_'.join(str(x)), axis=1)\n",
    "    \n",
    "    enc.fit(X_crossed)\n",
    "    X_crossed = enc.transform(X_crossed)\n",
    "    X_ints.append(X_crossed)\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints[-1]+1)\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32')\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "\n",
    "# merge the branches together\n",
    "final_branch = concatenate(all_branch_outputs)\n",
    "final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "# def create_model():\n",
    "#     model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "#     model.compile(optimizer='sgd',\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=1)\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=64)\n",
    "# results = cross_val_score(model, X_ints, df.gname.values, cv=kfold)\n",
    "\n",
    "# print(results.mean())\n",
    "## replace this with the train test pipeline\n",
    "model.fit(X_ints,\n",
    "          y_ints, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iyear', 'extended', 'vicinity', ..., 'claimmode_8.0',\n",
       "       'claimmode_9.0', 'claimmode_10.0'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "col_names = X.columns.values\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=64)\n",
    "for train_idx, test_idx in sss.split(X.values, y.values):\n",
    "    # X_train - 80% training attribute set\n",
    "    # X_test - 20% test attribute set\n",
    "    # y_train - 80% training labels\n",
    "    # y_test - 20% training labels\n",
    "    X_train, X_test = pd.DataFrame(X.values[train_idx], columns=col_names), pd.DataFrame(X.values[test_idx], columns=col_names)\n",
    "    y_train, y_test = pd.DataFrame(y.values[train_idx], columns=[\"propextent\"]), pd.DataFrame(y.values[test_idx], columns=[\"propextent\"])\n",
    "\n",
    "X_train, X_test = X_train.values, X_test.values\n",
    "y_train, y_test = y_train.values, y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 1.0, 2.0, 3.0}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 20)                68660     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 68,744\n",
      "Trainable params: 68,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JakeCarlson/anaconda/envs/MLEnv/lib/python3.6/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=3432, activation=\"relu\", units=20)`\n",
      "/Users/JakeCarlson/anaconda/envs/MLEnv/lib/python3.6/site-packages/ipykernel/__main__.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=4)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_37 to have shape (None, 4) but got array with shape (6094, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-0c0f04cb29d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# test on the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/MLEnv/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda/envs/MLEnv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1522\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/MLEnv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1379\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1381\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1382\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1383\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda/envs/MLEnv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_37 to have shape (None, 4) but got array with shape (6094, 1)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# ohe = LabelEncoder()\n",
    "# X_train_ohe = ohe.fit_transform(X_train[categorical_cols].values)\n",
    "# X_test_ohe = ohe.transform(X_test[categorical_cols].values)\n",
    "\n",
    "# inputs = Input(shape=(X_train.shape[1],),sparse=False)\n",
    "\n",
    "# # a layer instance is callable on a tensor, and returns a tensor\n",
    "# x = Dense(units=100, activation='relu')(inputs)\n",
    "# predictions = Dense(4,activation='softmax')(x)\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=input_dim, output_dim=20, activation='relu'))\n",
    "    model.add(Dense(output_dim=4, activation='softmax'))\n",
    "    model.compile(optimizer='sgd',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "# model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# model.compile(optimizer='sgd',\n",
    "#               loss='mean_squared_error',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model = build_model(X_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)\n",
    "\n",
    "# test on the data\n",
    "yhat = np.round(model.predict(X_test))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLEnv]",
   "language": "python",
   "name": "conda-env-MLEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
